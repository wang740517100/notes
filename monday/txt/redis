----------------------------------- 六种基本数据结构 -------------------------
String(字符串): 常用数据类型
List(列表): 链表实现, 可以实现简单的发布/订阅模式
set(集合): 过滤重复数据
zset(有序集合): 做排行榜
hash(哈希结构): 常用数据类型
HyperLogLog(基数): 做点赞数或网站访问率统计(3 5 7 5 7 8 -> 3 5 7 8 基数是4)

----------------------------------- 通信 -------------------------
gossip协议: meet, ping, pong, fail
优点: 去中心化的高可用性
缺点: 弱一致性

----------------------------------- 事务 ------------------------------
五个事务命令: MULTI(开启事务), EXEC(结束事务), DISCAR(取消事务)
WATCH(监听key, cas操作, 改变就会失败), UNWATCH(取消所有key监听)

Redis在执行事务命令时, 先将所有命令入队, 然后检测事务的命令是否正确, 如果不正确则报错, 也就是所有命令
都不会执行。只有当所有命令格式正确才会开始第二阶段执行。如果因为操作数据结构引起的错误, 则该命令执行出
现错误, 而其之前和之后的命令都会被正常执行(不会回滚)。


----------------------------------- rdb、aof和rewrite -------------------------
save 900 1
手动rdb: save(阻塞) 和 bgsave(非阻塞)
crontab定时任务做冷备

appendonly yes
appendfsync everysec
手动aof: bgrewriteaof

auto-aof-rewrite-percentage 100 #当前AOF大小膨胀到超过上次100%
auto-aof-rewrite-min-size 64mb #最小容量阈值
aof-rewrite-incremental-fsync yes #增量"文件同步"策略，每32M数据进行一次文件同步
no-appendfsync-on-rewrite yes #当 rewrite 时会阻塞disk的fsyn操作, 但把数据写进buffer照旧可能会丢失数据。

ps: rdb和aof不可同时进行, redis重启时优先使用aof数据恢复

----------------------------------- 主从同步 -------------------------
主从同步过程
1)slave节点发送 psyn 给master节点, master节点根据backlog里的信息对比master run id判断是进行全量复制还是增量复制。
2)全量复制: master节点fork子进程生成rdb文件传给slave节点, salve节点先将文件写入磁盘再加载进缓存中。
3)增量复制: master节点根据backlog里的replica offset将内存中缓存的操作命令传给salve节点执行更新。

- 异步复制: master每次接收到写命令之后, 先在内部写入数据, 然后异步发送给slave
- 过期key处理: slave不会过期key, 只会等待master过期key然后模拟一条del命令发送给slave。
- heartbeat: master默认每隔10秒, salve默认每隔1秒发送一个 heartbeat。
- repl-diskless-sync no #在内存中直接创建rdb发送给slave, 不会在本地磁盘存取。(无磁盘化复制)
- repl-diskless-sync-delay #等待一定时长再开始复制, 因为要等更多slave重新连接过来

----------------------------------- 主从切换 -------------------------
哨兵机制判断master宕机:
sdown: 如果一个哨兵ping一个master超过了指定的毫秒数之后,就主观认为master sdown(主观宕机)。
odown: 当quorum数量的哨兵判断一个master sdown了, 那么就是odown(客观宕机)。

执行主从切换的哨兵选举:
哨兵必须得到大部分哨兵(majority >= quorum(法定人数) ? majority : quorum )的授权, 才能执行切换

哨兵选举salve算法:
1)如果一个master被认为odown了, 某个哨兵就会执行主备切换选举一个slave来, 如果一个slave跟master断开连接已经超过了
down-after-milliseconds的10倍加上master宕机的时长,那么slave就被认为不适合选举为master。
2)接下来会对 slave 进行排序: 优先级、数据多的和稳定性好的。
- slave priority越低, 优先级就越高。
- replica offset大的, 代表复制了越多的数据
- 如果上面两个条件都相同, 那么选择一个run id比较小的那个slave(重启次数少,稳定)
3)主从切换后, 哨兵会负责自动纠正所有slave的一些配置。


----------------------------------- pub/sub 和 configuration epoch -------------------------

哨兵集群的自动发现机制: 哨兵使用pub/sub系统实现通信, 每个哨兵每隔两秒钟都会往自己监控的某个master+slaves对应
的channel里发送一个消息, 内容是自己的 host、ip和runid还有对这个master的监控配置, 每个哨兵还会跟其他哨兵交换对
master的监控配置, 互相进行监控配置的同步。


configuration epoch: 哨兵会对一套 redis master+slave进行监控, 执行切换的那个哨兵会从要切换到的 master那里得到
一个configuration epoch, 这就是一个version号, 每次切换的version号都必须是唯一的。如果第一个选举出的哨兵切换失败
了, 那么其他哨兵会等待failover-timeout 时间接替继续执行切换, 此时会重新获取一个新的configuration epoch, 作为新的
version号。

configuraiton传播: 哨兵完成切换之后, 会在自己本地更新生成最新的master配置, 然后同步给其他的哨兵, 就是通过之前说的
pub/sub 消息机制, 这里之前的version号就很重要了, 因为各种消息都是通过一个channel去发布和监听的, 所以一个哨兵完成一
次新的切换之后, 新的master配置是跟着新的version号的, 其他的哨兵都是根据版本号的大小来更新自己的master配置的。

----------------------------------- 脑裂和解决方法 ------------------------------------------------------
执行主从切换成功后, 宕机的master又复活了并可以接收客户端请求, 这时就会出现集群脑裂数据不一致问题。
解决方法:要求至少有1个slave数据复制和同步的延迟不能超过10秒
min-slaves-to-write 1
min-slaves-max-lag 10


-----------------------------------  redis的清理策略 -----------------------------------
1)定期删除: 定时任务定期清理过期key
2)惰性删除: 查询时发现过期就删除
3)淘汰机制: 一般使用allkeys-lru
- noeviction: 拒绝写操作, 读、删除可以正常使用。默认策略, 不建议使用
- allkeys-lru: 移除最近最少使用的key, 最常用的策略
- allkeys-random: 随机删除某个key, 不建议使用
- volatile-lru: 在设置了过期时间的key中, 移除最近最少使用的key, 不建议使用
- volatile-random: 在设置了过期时间的key中, 随机删除某个key, 不建议使用
- volatile-ttl: 在设置了过期时间的key中, 把最早要过期的key优先删除


----------------------------------- 一致性哈希 ---------------------------------------------------
redis cluster支持动态添加master节点, 只需要重新分配slot即可(对比es就不能动态添加shard),
原因就是redis使用了一致性hash。
一致性哈希结构: 环形结构和虚拟节点(slot)
环形结构: 解决单台机器宕机后, 该台机器上的大量请求直接请求db打死数据库。
虚拟节点: 使hash值均匀分布。


----------------------------------- 分布式锁语法 --------------------------------------------------

SETNX key value #设置成功返回1, 设置失败返回0。

----------------------------------- 实际问题 --------------------------------------------------
1)缓存雪崩: redis集群彻底崩溃后，大量请求直接打到mysql上面，导致mysql也崩溃继而导致源服务乃至整个网站崩溃。
解决方案：
事前: 发生缓存雪崩之前怎么防止redis挂掉
- redis集群部署(冗余slave部署)，利用redis本身的主从切换的高可用性
- 双机房部署，一套集群部署在两个机房或两个机房各自部署一套集群

事中: redis集群已挂
- 三层缓存架构，nginx本地缓存 + redis 集群 + ehcache 本地缓存服务
- 对redis集群访问进行资源隔离和提供降级处理，防止大量请求超时缓存服务崩溃
- 对源服务的访问进行限流和资源隔离，防止大量请求涌入源服务导致其挂掉

事后：redis 集群重启时
- 对redis做热备和冷备(RDB 和 AOF)，快速启动redis集群
- 启动时对redis做快速预热，防止启动后再次崩溃

2)缓存穿透: 大量请求在穿过缓存直接访问redis且每次执行结果都为空, 导致各层缓存机制失效
当请求到达一定大时就会把mysql打死。
解决方案：
1)把查询结果为空的数据也放在 redis 集群 + 缓存服务 + nginx 里面;
2)布隆过滤器

3)缓存失效: 大量缓存设置了同一过期时间失效，导致大量请求突然穿过缓存直接访问后端服务，大量的网络请求导致网络负载加重或者服务挂掉.
解决方案：在 nginx 端给缓存设置一定范围内的随机过期时间避免大量缓存数据在同一时间过期

4)缓存和db双写一致性问题: 使用堆内存队列

5)缓存预热和热点数据:
1)利用storm做实时热点数据统计导入到redis集群。
2)把热点数据保存在所有节点上，做负载均衡。
3)使用三级缓存架构。
4)引入阿里的Tair组件。

----------------------------------- 布隆过滤器 -----------------------------------
布隆过滤器: 它是一种基于概率的数据结构,主要用来判断某个元素是否在集合内。
优点: 空间效率和查询效率高, 不存在漏报
缺点: 存在误报和删除困难
False positive: 误判,集合里没有某元素,查找结果是有该元素。
False Negative: 漏判,集合里有某元素,查找结果是没有该元素。

实现原理:
- m : bit数组的宽度(bit数)
- n : 加入其中的key的数量
- k : 使用的hash函数的个数
https://blog.csdn.net/CrankZ/article/details/84928562

----------------------------------- redis与memcached的区别 -----------------------------------
1)redis的数据结构多。
2)对于小kv(100k以下),redis性能好。
3)redis支持集群模式。


----------------------------------- redis的单线程模型 -----------------------------------
server socket (s) ->  IO多路复用程序  -> 队列 -> 文件事件分派器 -> 处理器(连接应答处理器、命令请求处理器、命令回复处理器)
- 一个线程处理一个队列里面的信息
- 非阻塞IO多路复用请求,将请求压到队列里面,真正处理请求的是处理器
- 基于内存处理请求所以快